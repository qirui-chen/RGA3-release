{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "#[VideoRefer-7B, VideoLLaMA3, Qwen2.5-VL-7B-Instruct, Qwen2.5-VL-3B-Instruct, Osprey-7B, GPT-4o-low-8frames]\n",
    "result_root = \"/PATH/TO/results/RefVideoQA/GPT-4o-low-8frames\"\n",
    "\n",
    "API_KEY = \"\"\n",
    "\n",
    "eval_result = json.load(open(os.path.join(result_root, \"eval_result.json\")))\n",
    "requests = []\n",
    "\n",
    "for vid in eval_result:\n",
    "    for exp_id in eval_result[vid]:\n",
    "        for qa_id in eval_result[vid][exp_id]:\n",
    "            task_id = f\"{vid}____{exp_id}____{qa_id}\"\n",
    "\n",
    "            sample = eval_result[vid][exp_id][qa_id]\n",
    "            question, gt_answer, pred_answer = (\n",
    "                sample[\"question\"],\n",
    "                sample[\"gt_answer\"],\n",
    "                sample[\"pred_answer\"],\n",
    "            )\n",
    "\n",
    "            request = {\n",
    "                \"custom_id\": task_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o-batch\",\n",
    "                    \"response_format\": {\"type\": \"json_object\"},\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \n",
    "                            \"You are an intelligent chatbot designed for evaluating the correctness of generative outputs for question-answer pairs. \"\n",
    "                            \"Your response should be in JSON format.\"\n",
    "                            \"Your task is to compare the predicted answer with the correct answer and determine if they match meaningfully. Here's how you can accomplish the task:\"\n",
    "                            \"------\"\n",
    "                            \"##INSTRUCTIONS: \"\n",
    "                            \"- Focus on the meaningful match between the predicted answer and the correct answer.\\n\"\n",
    "                            \"- Consider synonyms or paraphrases as valid matches.\\n\"\n",
    "                            \"- Evaluate the correctness of the prediction compared to the answer.\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": \n",
    "                            \"Please evaluate the following video-based question-answer pair:\\n\\n\"\n",
    "                            f\"Question: {question}\\n\"\n",
    "                            f\"Correct Answer: {gt_answer}\\n\"\n",
    "                            f\"Predicted Answer: {pred_answer}\\n\\n\"\n",
    "                            \"Provide your evaluation only as a yes/no and score where the score is an integer value between 0 and 5, with 5 indicating the highest meaningful match. \"\n",
    "                            \"Please generate the response in the form of a Python dictionary string with keys 'pred' and 'score', where value of 'pred' is  a string of 'yes' or 'no' and value of 'score' is in INTEGER, not STRING.\"\n",
    "                            \"DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string. \"\n",
    "                            \"For example, your response should look like this: {'pred': 'yes', 'score': 4.8}.\"\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            }\n",
    "\n",
    "            requests.append(request)\n",
    "\n",
    "requests_path = os.path.join(result_root, \"requests.jsonl\")\n",
    "with open(requests_path, \"w\") as f:\n",
    "    for request in requests:\n",
    "        json.dump(request, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_command = f'curl --location \"https://runway.devops.xiaohongshu.com/openai/azure/upload_batch_file?api-version=2024-10-01-preview\" ' \\\n",
    "          f'--header \"api-key: {API_KEY}\" ' \\\n",
    "          f'--form \"purpose=batch\" ' \\\n",
    "          f'--form \"file=@{requests_path}\"'\n",
    "\n",
    "\n",
    "push_result = !{push_command}\n",
    "\n",
    "\n",
    "json_str = ''.join(push_result[-9:])\n",
    "file_id = eval(json_str)['id']\n",
    "file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_command = f'''curl --location \"https://runway.devops.xiaohongshu.com/openai/azure/batches/create?api-version=2024-10-01-preview\" \\\n",
    "          --header \"api-key: {API_KEY}\" \\\n",
    "          --header \"Content-Type: application/json\" \\\n",
    "          --data '{{\"input_file_id\": \"{file_id}\", \"endpoint\": \"/chat/completions\", \"completion_window\": \"24h\"}}' '''\n",
    "\n",
    "submit_result = !{submit_command}\n",
    "\n",
    "\n",
    "response_text = ''.join(submit_result)\n",
    "match = re.search(r'\"id\":\\s*\"([^\"]+)\"', response_text)\n",
    "batch_id = match.group(1)\n",
    "print(batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_command = f'''curl --location \"https://runway.devops.xiaohongshu.com/openai/azure/batches/{batch_id}?api-version=2024-10-01-preview\" \\\n",
    "                --header \"api-key: {API_KEY}\"'''\n",
    "\n",
    "query_result = !{query_command}\n",
    "\n",
    "match = re.search(r'\"output_file_id\":\\s*\"([^\"]*)\"', ''.join(query_result))\n",
    "output_file_id = match.group(1)\n",
    "\n",
    "query_result, output_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(result_root, 'outputs.jsonl')\n",
    "pull_command = f'''curl \"https://runway.devops.xiaohongshu.com/openai/azure/out_batch_file/{output_file_id}?api-version=2024-10-01-preview\" \\\n",
    "             -H \"api-key: {API_KEY}\"  > {save_path}'''\n",
    "!{pull_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "output = []\n",
    "with open(save_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            output.append(json.loads(line))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "eval_result_gpt = eval_result.copy()\n",
    "\n",
    "\n",
    "# Calculate average score and accuracy\n",
    "score_sum = 0\n",
    "count = 0\n",
    "yes_count = 0\n",
    "no_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for sample in tqdm(output):\n",
    "\n",
    "    vid, exp_id, qa_id = sample['custom_id'].split('____')\n",
    "    if 'content' not in sample[\"response\"][\"body\"]['choices'][0]['message'].keys():\n",
    "        continue\n",
    "    result = sample[\"response\"][\"body\"]['choices'][0]['message']['content']\n",
    "    \n",
    "    try:\n",
    "        result = eval(result)\n",
    "\n",
    "        # Computing score\n",
    "        if 'score' in result:\n",
    "            count += 1\n",
    "            score = max(min(int(result['score']), 5), 0)\n",
    "            eval_result_gpt[vid][exp_id][qa_id]['score'] = score\n",
    "            score_sum += score\n",
    "\n",
    "        # Computing accuracy\n",
    "        if 'pred' in result:\n",
    "            pred = result['pred']\n",
    "            eval_result_gpt[vid][exp_id][qa_id]['pred'] = pred\n",
    "            if \"yes\" in pred.lower():\n",
    "                yes_count += 1\n",
    "            elif \"no\" in pred.lower():\n",
    "                no_count += 1\n",
    "    except:\n",
    "        error_count += 1\n",
    "        continue\n",
    "\n",
    "\n",
    "average_score = score_sum / count\n",
    "accuracy = yes_count / (yes_count + no_count)\n",
    "print(\"Yes count:\", yes_count)\n",
    "print(\"No count:\", no_count)\n",
    "print(\"Error count:\", error_count)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Average score:\", average_score)\n",
    "\n",
    "# eval_result_gpt= {\n",
    "#     \"Yes count\": yes_count,\n",
    "#     \"No count\": no_count,\n",
    "#     \"Accuracy\": accuracy,\n",
    "#     \"Average score\": average_score,\n",
    "#     **eval_result_gpt\n",
    "# }\n",
    "with open(os.path.join(result_root, \"eval_result_gpt.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(eval_result_gpt, file, indent=2, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
